{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "glH4LZEkyIVj",
        "ihJo4wAsxr1_"
      ],
      "private_outputs": true,
      "mount_file_id": "15L5pAceN_FPNqX-rGy3OmsTics9JjPVz",
      "authorship_tag": "ABX9TyMnbvfVStreaiRJKsZZe/uO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pamslover/pamellaPfe/blob/main/Fine_tuningModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "RX7pugkcZA1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "LW5mPvEfY-Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "id": "ooZvighD3J1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim pyLDAvis\n"
      ],
      "metadata": {
        "id": "wAIHPle_zAhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "haA9Gq1BFWQE"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JoImUzpoC0HC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdpqbAh8_Q_E"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import torch"
      ],
      "metadata": {
        "id": "5--Ig0niF4H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA"
      ],
      "metadata": {
        "id": "gOat-pHpaJDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import pandas as pd\n",
        "\n",
        "# Charger les CVs\n",
        "data_cv = pd.read_csv(\"/content/drive/MyDrive/projet pfe/CvClean.csv\")\n",
        "\n",
        "# Tokenisation et prétraitement\n",
        "texts = [text.split() for text in data_cv['Resume']]\n",
        "\n",
        "# Création du dictionnaire et du corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Appliquer LDA\n",
        "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "\n",
        "# Afficher les thèmes\n",
        "topics = lda_model.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "id": "XKJdZ-7MaLol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# Visualisation\n",
        "vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis)\n"
      ],
      "metadata": {
        "id": "g8bKjwNWaLrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYhg_pZIaLuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXD7gHRt_UIo"
      },
      "source": [
        "# word2vec et bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8SsQDnK_fCT"
      },
      "source": [
        "## Fine-tuning de word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-FANR2w-tr2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/projet pfe/CvClean.csv')\n",
        "jd = pd.read_csv(\"/content/drive/MyDrive/projet pfe/job.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langid.langid import LanguageIdentifier, model\n",
        "\n",
        "# detection de la langue de texte\n",
        "class LanguageCheck:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text = None\n",
        "        self.language = None\n",
        "\n",
        "    def override(self, text=None):\n",
        "        self.text = text\n",
        "        identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
        "        self.language, _ = identifier.classify(self.text)\n",
        "\n",
        "        return self.language"
      ],
      "metadata": {
        "id": "0QX7owpX3iNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rbSlfvf7WI82"
      },
      "outputs": [],
      "source": [
        "cvs = []\n",
        "i = 0\n",
        "for i in tqdm(range(len(df['Resume']))):\n",
        "  text = str(df['Resume'][i]).strip().lower()\n",
        "  tokens = word_tokenize(text) # Tokenize the text\n",
        "  txt = [token for token in tokens if not token in stopwords.words('english') and token.isalpha()] # Use stopwords.words()\n",
        "  txt = ' '.join(w for w in txt)\n",
        "  cvs.append(txt)\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o15y5dQYjmjI"
      },
      "outputs": [],
      "source": [
        "\n",
        "jobs = []\n",
        "i = 0\n",
        "for i in tqdm(range(len(jd['job']))):\n",
        "  text = str(jd['job'][i]).strip().lower()\n",
        "  check = LanguageCheck()\n",
        "  lang = check.override(text)\n",
        "  tokens = word_tokenize(text)\n",
        "  if lang == 'fr':\n",
        "    txt = [token for token in tokens if not token in stopwords.words('french') and token.isalpha()] # Use stopwords.words()\n",
        "    txt = ' '.join(w for w in txt)\n",
        "  else:\n",
        "    txt = [token for token in tokens if not token in stopwords.words('english') and token.isalpha()] # Use stopwords.words()\n",
        "    txt = ' '.join(w for w in txt)\n",
        "  jobs.append(txt)\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es9_gJ_E_Tf_"
      },
      "outputs": [],
      "source": [
        "corpus = cvs + jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccj_pcfH6_h5"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "sentences_tokenized = [str(w).lower() for w in corpus]\n",
        "sentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvSOnNgO-9UG"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/projet pfe/word2vec/GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "model = Word2Vec(min_count=1, vector_size=300)\n",
        "model.build_vocab(sentences_tokenized)\n",
        "model.wv.vectors_lockf = np.ones(len(model.wv))\n",
        "total_exemple = model.corpus_count\n",
        "model.wv.intersect_word2vec_format(path, binary=True, lockf=1.0)\n",
        "model.train(sentences_tokenized, total_examples=total_exemple, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCk668uD_DAt"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/projet pfe/word2vec/word2vec_gensim_Cv_job.model\")\n",
        "\n",
        "model.wv.save_word2vec_format(\"/content/drive/MyDrive/projet pfe/word2vec/word2vec_cv_job.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csjnydMl6_op"
      },
      "outputs": [],
      "source": [
        "w2v =KeyedVectors.load(\"/content/drive/MyDrive/projet pfe/word2vec/word2vec_gensim_Cv_job.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NiTnBrunAhT_"
      },
      "outputs": [],
      "source": [
        "w2v.wv.most_similar(\"nlp\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AM0S6lGpD3bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oLr4dI8Ah9K"
      },
      "source": [
        "## Fine-tuning de BERT sur les CVs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HzE6WON26_tc"
      },
      "outputs": [],
      "source": [
        "corpus_cv = df['Resume'].tolist()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(str(corpus_cv), return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "\n",
        "# Créer un Dataset et un DataLoader\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        return item\n",
        "\n",
        "dataset = TextDataset(inputs)\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # sortie du répertoire\n",
        "    overwrite_output_dir=True,       # écraser le contenu du répertoire de sortie\n",
        "    num_train_epochs=3,              # nombre d'époques d'entraînement\n",
        "    per_device_train_batch_size=8,   # taille du lot d'entraînement\n",
        "    save_steps=10_000,               # sauvegarder les modèles tous les 10 000 pas\n",
        "    save_total_limit=2,              # ne conserver que les 2 derniers modèles sauvegardés\n",
        ")\n",
        "\n",
        "# Créer l'entraîneur\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# Entraîner le modèle\n",
        "trainer.train()\n",
        "\n",
        "# Sauvegarder le modèle fine-tuné\n",
        "model.save_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/model_bert_cv\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/token_bert_cv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek576tPewwb7"
      },
      "source": [
        "## Fine-tuning de BERT sur les offres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YX_nV5cWMlO3"
      },
      "outputs": [],
      "source": [
        "corpus_jb = jd['job'].tolist()\n",
        "\n",
        "# Télécharger le tokenizer et le modèle pré-entraîné\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(str(corpus_jb), return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        return item\n",
        "\n",
        "dataset = TextDataset(inputs)\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # sortie du répertoire\n",
        "    overwrite_output_dir=True,       # écraser le contenu du répertoire de sortie\n",
        "    num_train_epochs=3,              # nombre d'époques d'entraînement\n",
        "    per_device_train_batch_size=8,   # taille du lot d'entraînement\n",
        "    save_steps=10_000,               # sauvegarder les modèles tous les 10 000 pas\n",
        "    save_total_limit=2,              # ne conserver que les 2 derniers modèles sauvegardés\n",
        ")\n",
        "\n",
        "# Créer l'entraîneur\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# Entraîner le modèle\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Sauvegarde du modele\n",
        "model.save_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/model_bert_job\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/token_bert_job\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41qEHXNgw8Dz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYUNRj_aGF3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA"
      ],
      "metadata": {
        "id": "W-iQ0M_L1Zy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Charger les données (ajustez les chemins et les noms de colonnes en fonction de vos données)\n",
        "df = pd.read_csv('/content/drive/MyDrive/projet pfe/CvClean.csv')\n",
        "jd = pd.read_csv(\"/content/drive/MyDrive/projet pfe/job.csv\")\n",
        "\n",
        "# Fusionner les CVs et les offres d'emploi pour créer un corpus unique\n",
        "documents = df['Resume'].tolist() + jd['job'].tolist()\n",
        "\n",
        "corpuss = jobs + cvs\n"
      ],
      "metadata": {
        "id": "XtFNwnmx1dOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un dictionnaire\n",
        "dictionary = corpora.Dictionary(corpuss)\n",
        "\n",
        "# Filtrer les tokens rares et fréquents\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "\n",
        "# Créer un corpus (bag of words)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in corpuss]\n"
      ],
      "metadata": {
        "id": "2IV9_Pg31daQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "# Définir le nombre de thèmes\n",
        "num_topics = 10\n",
        "\n",
        "# Entraîner le modèle LDA\n",
        "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n"
      ],
      "metadata": {
        "id": "B11L0Rqa3hSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenir les distributions de thèmes pour les CVs\n",
        "corpus_cv = [dictionary.doc2bow(preprocess(doc)) for doc in df_cv['Resume'].tolist()]\n",
        "lda_cv = [lda_model.get_document_topics(bow) for bow in corpus_cv]\n",
        "\n",
        "# Obtenir les distributions de thèmes pour les offres d'emploi\n",
        "corpus_job = [dictionary.doc2bow(preprocess(doc)) for doc in df_job['JobDescription'].tolist()]\n",
        "lda_job = [lda_model.get_document_topics(bow) for bow in corpus_job]\n",
        "\n",
        "# Convertir les distributions de thèmes en matrices numpy\n",
        "import numpy as np\n",
        "\n",
        "def topics_to_matrix(lda_topics, num_topics):\n",
        "    matrix = np.zeros((len(lda_topics), num_topics))\n",
        "    for i, topics in enumerate(lda_topics):\n",
        "        for topic, prob in topics:\n",
        "            matrix[i, topic] = prob\n",
        "    return matrix\n",
        "\n",
        "lda_topic_distributions_cv = topics_to_matrix(lda_cv, num_topics)\n",
        "lda_topic_distributions_job = topics_to_matrix(lda_job, num_topics)\n",
        "\n",
        "# Afficher les distributions de thèmes pour le premier CV et la première offre d'emploi\n",
        "print(lda_topic_distributions_cv[0])\n",
        "print(lda_topic_distributions_job[0])\n"
      ],
      "metadata": {
        "id": "OUWeEtch3hXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple d'embeddings BERT ou Word2Vec (remplacez par vos propres embeddings)\n",
        "bert_embeddings_cv = np.random.rand(len(df_cv), 768)\n",
        "bert_embeddings_job = np.random.rand(len(df_job), 768)\n",
        "\n",
        "# Fusionner les embeddings avec les proportions de thèmes\n",
        "combined_embeddings_cv = np.hstack((bert_embeddings_cv, lda_topic_distributions_cv))\n",
        "combined_embeddings_job = np.hstack((bert_embeddings_job, lda_topic_distributions_job))\n"
      ],
      "metadata": {
        "id": "VxKfzrCn3t5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Appliquer KMeans sur les proportions de thèmes LDA\n",
        "num_clusters = 10  # Définir le nombre de clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "clusters_cv = kmeans.fit_predict(lda_topic_distributions_cv)\n",
        "clusters_job = kmeans.fit_predict(lda_topic_distributions_job)\n"
      ],
      "metadata": {
        "id": "qogu1z8i3t9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxNJByne3uAq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}