{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pamslover/pamellaPfe/blob/hotfix/clustering_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "AGZS57teZKHN"
      },
      "id": "AGZS57teZKHN",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9euqinXowQC7",
        "outputId": "3e8cac92-9b55-4588-8cc6-00475714a422"
      },
      "id": "9euqinXowQC7",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m900.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake) (1.25.2)\n",
            "Collecting segtok (from yake)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (1.0.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake) (2024.5.15)\n",
            "Installing collected packages: segtok, yake\n",
            "Successfully installed segtok-1.5.11 yake-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import SpectralClustering\n",
        "import itertools\n",
        "from math import sqrt, floor\n",
        "import os\n",
        "import re\n",
        "from os import path\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import yake\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "mFxPOWgDIZZf"
      },
      "execution_count": 5,
      "outputs": [],
      "id": "mFxPOWgDIZZf"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "oPIDxXU4dDmo"
      },
      "id": "oPIDxXU4dDmo",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yake\n",
        "nous allons utiliser yake pour extraction des mots clés dans nos cv et job pour le clustering de notre corpus"
      ],
      "metadata": {
        "id": "5Aollt9-bcLG"
      },
      "id": "5Aollt9-bcLG"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1e67d4ef",
      "metadata": {
        "papermill": {
          "duration": 0.124198,
          "end_time": "2021-12-24T10:25:09.663388",
          "exception": false,
          "start_time": "2021-12-24T10:25:09.539190",
          "status": "completed"
        },
        "tags": [],
        "id": "1e67d4ef"
      },
      "outputs": [],
      "source": [
        "def keywords_yake(sample_posts):\n",
        "    # take keywords for each post & turn them into a text string \"sentence\"\n",
        "    # create empty list to save our \"sentences\" to\n",
        "    simple_kw_extractor = yake.KeywordExtractor()\n",
        "    sentences = []\n",
        "    for post in sample_posts:\n",
        "        post_keyword = simple_kw_extractor.extract_keywords(post)\n",
        "        sentence_output = \"\"\n",
        "        for word, number in post_keyword:\n",
        "            sentence_output += word + \" \"\n",
        "        sentences.append(sentence_output)\n",
        "    return sentences\n",
        "\n",
        "    # use the sentences as input for brown clustering\n",
        "\n",
        "\n",
        "def tokenizing_after_yake(sentences):\n",
        "    # tokenize\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    sample_data_tokenized = [w.lower() for w in sentences]\n",
        "    sample_data_tokenized = [tokenizer.tokenize(i) for i in sample_data_tokenized]\n",
        "    return sample_data_tokenized\n",
        "\n",
        "\n",
        "class YakeHelperFunctions:\n",
        "\n",
        "    def get_cluster_maybe(self, megacluster):\n",
        "        cluster_list = [[]]\n",
        "        list_index = 0\n",
        "        for i in range(len(megacluster) - 1):\n",
        "            if megacluster[i] < megacluster[i + 1]:\n",
        "                cluster_list[list_index].append(megacluster[i])\n",
        "            else:\n",
        "                cluster_list[list_index].append(megacluster[i])\n",
        "                list_index = list_index + 1\n",
        "                cluster_list.append([])\n",
        "        return cluster_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOsAHcprw8os"
      },
      "source": [
        "# Clustering"
      ],
      "id": "IOsAHcprw8os"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectralclustering"
      ],
      "metadata": {
        "id": "dnVqJg32cIQm"
      },
      "id": "dnVqJg32cIQm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clustering_embedding_word2vec"
      ],
      "metadata": {
        "id": "AxjlRqCv_tm1"
      },
      "id": "AxjlRqCv_tm1"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/projet pfe/CvClean.csv')\n",
        "jd = pd.read_csv(\"/content/drive/MyDrive/projet pfe/job.csv\")"
      ],
      "metadata": {
        "id": "6rSWJW3uD4uu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "f2737097-a6c5-494e-a6c7-65f740d92141"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/projet pfe/CvClean.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-93e5711a1579>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/projet pfe/CvClean.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/projet pfe/job.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/projet pfe/CvClean.csv'"
          ]
        }
      ],
      "id": "6rSWJW3uD4uu"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3nwGXIdwtlw",
        "outputId": "12dbeebc-fb76-4b91-c3b8-d91048190433"
      },
      "id": "v3nwGXIdwtlw",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5wq1HazwP7W"
      },
      "id": "J5wq1HazwP7W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df['Resume'].astype(str)"
      ],
      "metadata": {
        "id": "JxwXy23lJBhV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JxwXy23lJBhV"
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(0, inplace=True)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "BFc0QAC3MDhx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BFc0QAC3MDhx"
    },
    {
      "cell_type": "code",
      "source": [
        "jd.shape"
      ],
      "metadata": {
        "id": "48Hfe-G5A9X6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "48Hfe-G5A9X6"
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = pd.read_csv(\"/content/drive/MyDrive/projet pfe/word2vec/word2vec_cv_job.model\", delim_whitespace=True, skiprows=[0], header=None)"
      ],
      "metadata": {
        "id": "_6mVfw3K_ijf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_6mVfw3K_ijf"
    },
    {
      "cell_type": "code",
      "source": [
        "vectors"
      ],
      "metadata": {
        "id": "WozrqKqXB7sZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WozrqKqXB7sZ"
    },
    {
      "cell_type": "code",
      "source": [
        "#extraction des mots clés avec yake\n",
        "keywords = keywords_yake(df['Resume'].astype(str))\n",
        "keywords_tokenized = tokenizing_after_yake(keywords)\n",
        "keywords_set = [ set(job) for job in keywords_tokenized]\n",
        "#remove empty sets\n",
        "keywords_set_no_empty = [x for x in keywords_set if x]"
      ],
      "metadata": {
        "id": "e0TVoBeH_itF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "e0TVoBeH_itF"
    },
    {
      "cell_type": "code",
      "source": [
        "def vectors_from_job(job):\n",
        "        all_words = []\n",
        "        for words in job:\n",
        "                all_words.append(words)\n",
        "        return vectors[vectors.index.isin(all_words)]"
      ],
      "metadata": {
        "id": "eP7XT7HaBMeZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eP7XT7HaBMeZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# test function for getting job description embedding (in the dumbest way possible)\n",
        "def doc_embed_from_job(job):\n",
        "        test_vectors = vectors_from_job(job)\n",
        "        if test_vectors.empty:\n",
        "            return np.zeros(300) # Return a zero vector if no embeddings are found\n",
        "        else:\n",
        "            return test_vectors.mean()"
      ],
      "metadata": {
        "id": "1CDkIcWdBMiX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1CDkIcWdBMiX"
    },
    {
      "cell_type": "code",
      "source": [
        "# get document embeddings for job\n",
        "num_of_jobs = len(keywords_set_no_empty)\n",
        "doc_embeddings = np.zeros([num_of_jobs,300])\n",
        "# TODO: handle jobs where all word out vocabulary\n",
        "for i in range(num_of_jobs):\n",
        "        embeddings =np.array(doc_embed_from_job(keywords_set_no_empty[i]))\n",
        "        if np.isnan(embeddings).any():\n",
        "                doc_embeddings[i,:] = np.zeros([1,300])\n",
        "        else:\n",
        "                doc_embeddings[i,:] = embeddings"
      ],
      "metadata": {
        "id": "aHWiap0kBMnK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aHWiap0kBMnK"
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embeddings.shape"
      ],
      "metadata": {
        "id": "5iFFdB7eBM24"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5iFFdB7eBM24"
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_clusters = floor(sqrt(num_of_jobs))"
      ],
      "metadata": {
        "id": "Sy6pUOr8CQel"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Sy6pUOr8CQel"
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = SpectralClustering(assign_labels='discretize', random_state=42, n_neighbors=number_of_clusters).fit(doc_embeddings)"
      ],
      "metadata": {
        "id": "hgVF4G6xCQ7_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hgVF4G6xCQ7_"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(clustering.labels_).value_counts()"
      ],
      "metadata": {
        "id": "CJlbEVByCQ_o"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CJlbEVByCQ_o"
    },
    {
      "cell_type": "code",
      "source": [
        "# explore our jobs by cluster number\n",
        "jobs_subset = keywords_set_no_empty[0:num_of_jobs]\n",
        "def get_keyword_set_by_cluster_number(number):\n",
        "        cluster_index = list(clustering.labels_ == number)\n",
        "        return list(itertools.compress(jobs_subset, cluster_index))"
      ],
      "metadata": {
        "id": "9k8go8TBCRE7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9k8go8TBCRE7"
    },
    {
      "cell_type": "code",
      "source": [
        "get_keyword_set_by_cluster_number(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2Kodv5qNCfsf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2Kodv5qNCfsf"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_job_infos_by_cluster_number(number,  data, cluster):\n",
        "        if len(cluster.labels_) != len(data):\n",
        "            print(f\"Length of labels: {len(cluster.labels_)}\")\n",
        "            print(f\"Length of data: {len(data)}\")\n",
        "            raise ValueError(\"Number of labels does not match the number of data rows.\")\n",
        "        return data[cluster.labels_ == number]\n",
        "\n",
        "def remove_unnecessary_word(text, liste_word):\n",
        "        text = text.lower()\n",
        "        for element in liste_word:\n",
        "                text = re.sub(element,\"\",text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "-bRVl2s0CfwZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-bRVl2s0CfwZ"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_of_jobs):\n",
        "        print(f\"cluster {i}:\\n\")\n",
        "        print(get_job_infos_by_cluster_number(i, data, clustering))\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "AZssBKyjCfz1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AZssBKyjCfz1"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(number_of_clusters):\n",
        "        jobs_as_string = get_job_infos_by_cluster_number(i, data, clustering)\\\n",
        "        .to_string(index=True)\n",
        "        jobs_as_string = remove_unnecessary_word(jobs_as_string,[])\n",
        "        # Generate a word cloud image\n",
        "        wordcloud = WordCloud().generate(jobs_as_string)\n",
        "\n",
        "        # Display the generated image:\n",
        "        # the matplotlib way:\n",
        "\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "_sig3aasCf3d"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_sig3aasCf3d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "refining cluster"
      ],
      "metadata": {
        "id": "tXMPbp69C2b4"
      },
      "id": "tXMPbp69C2b4"
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_counts = pd.Series(clustering.labels_).value_counts()\n",
        "max_cluster_size = number_of_clusters * 2\n",
        "big_clusters = cluster_counts[cluster_counts > max_cluster_size]"
      ],
      "metadata": {
        "id": "6JpFckuBCvbK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6JpFckuBCvbK"
    },
    {
      "cell_type": "code",
      "source": [
        "big_clusters"
      ],
      "metadata": {
        "id": "m_CLaCY_CvXn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m_CLaCY_CvXn"
    },
    {
      "cell_type": "code",
      "source": [
        "# first\n",
        "cluster_label = big_clusters.index[0]\n",
        "sub_sample = data[clustering.labels_ == cluster_label]\n",
        "sub_cluster_embeddings = doc_embeddings[clustering.labels_ == cluster_label]"
      ],
      "metadata": {
        "id": "p3CAHPVCCvTI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "p3CAHPVCCvTI"
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_sub_clusters = floor(sqrt(sub_sample.shape[0]))"
      ],
      "metadata": {
        "id": "84imA1IWCvDc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "84imA1IWCvDc"
    },
    {
      "cell_type": "code",
      "source": [
        "sub_clustering = SpectralClustering(assign_labels='discretize', random_state=42, n_neighbors=number_of_sub_clusters, n_clusters=number_of_sub_clusters).fit(sub_cluster_embeddings)"
      ],
      "metadata": {
        "id": "1WYDbLfMDJeP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1WYDbLfMDJeP"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(number_of_sub_clusters):\n",
        "        print(f\"cluster {i}:\\n\")\n",
        "        print(get_job_infos_by_cluster_number(i, sub_sample, sub_clustering))\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "02AF5lJqDJaw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "02AF5lJqDJaw"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(sub_clustering.labels_).value_counts()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wt82iD6uDJUm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wt82iD6uDJUm"
    },
    {
      "cell_type": "code",
      "source": [
        "get_job_infos_by_cluster_number(4, sub_sample, sub_clustering)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VLxLCh8GBM72"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VLxLCh8GBM72"
    },
    {
      "cell_type": "code",
      "source": [
        "jobs_from_cluster = get_job_infos_by_cluster_number(4, sub_sample, sub_clustering)\n",
        "jobs_as_string = jobs_from_cluster.to_string(index=True)"
      ],
      "metadata": {
        "id": "FoVrb3q-DY-7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FoVrb3q-DY-7"
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud().generate(jobs_as_string)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5SfoBSHzDY7M"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5SfoBSHzDY7M"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(number_of_sub_clusters):\n",
        "        jobs_as_string = get_job_infos_by_cluster_number(i, sub_sample, sub_clustering)\\\n",
        "        .to_string(index=True)\n",
        "        # Generate a word cloud image\n",
        "        wordcloud = WordCloud().generate(jobs_as_string)\n",
        "\n",
        "        # Display the generated image:\n",
        "        # the matplotlib way:\n",
        "\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ud8rVmg5DY3_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ud8rVmg5DY3_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPmYf2UgDYjR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MPmYf2UgDYjR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clustering_embedding_bert"
      ],
      "metadata": {
        "id": "Skbpv4sbP_fw"
      },
      "id": "Skbpv4sbP_fw"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/projet pfe/CvClean.csv')\n",
        "jd = pd.read_csv(\"/content/drive/MyDrive/projet pfe/job.csv\")\n",
        "# Charger les modèles et les tokenizers fine-tunés\n",
        "cv_model = BertModel.from_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/model_bert_cv\")\n",
        "cv_tokenizer = BertTokenizer.from_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/token_bert_cv\")\n",
        "\n",
        "job_model = BertModel.from_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/model_bert_job\")\n",
        "job_tokenizer = BertTokenizer.from_pretrained(\"/content/drive/MyDrive/projet pfe/BERT/token_bert_job\")\n"
      ],
      "metadata": {
        "id": "7mgg3yOdDYfH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7mgg3yOdDYfH"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKpqdpCrdBZQ"
      },
      "id": "QKpqdpCrdBZQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_embeddings(texts, model, tokenizer, max_length=512):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', max_length=max_length, truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1)"
      ],
      "metadata": {
        "id": "WUvMRJy9QTu7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WUvMRJy9QTu7"
    },
    {
      "cell_type": "code",
      "source": [
        "samples_cv = df['Resume'].astype(str)\n",
        "samples_job = jd['job'].astype(str)\n",
        "\n",
        "job_embeddings = get_embeddings(samples_job.tolist(), job_model, job_tokenizer)\n",
        "cv_embeddings = get_embeddings(samples_cv.tolist(), cv_model, cv_tokenizer)\n",
        "\n",
        "all_embeddings = torch.cat((cv_embeddings, job_embeddings), dim=0)\n",
        "\n",
        "# Définir le nombre de clusters\n",
        "num_clusters = floor(sqrt(len(all_embeddings)))"
      ],
      "metadata": {
        "id": "BSxPoAKsQcZ2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BSxPoAKsQcZ2"
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = SpectralClustering(assign_labels='discretize', random_state=42, n_clusters=num_clusters).fit(all_embeddings)"
      ],
      "metadata": {
        "id": "PocnehnPRIxE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PocnehnPRIxE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation des clusters avec t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_embeddings = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "colors = ['red'] * len(samples_cv) + ['blue'] * len(samples_job)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=colors, alpha=0.6)\n",
        "plt.title(\"Clustering des Embeddings des CVs et Offres d'Emploi\")\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HO8r0p7sQTyq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HO8r0p7sQTyq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les clusters et générer des nuages de mots\n",
        "def get_job_infos_by_cluster_number(number, data, cluster):\n",
        "    return data[cluster.labels_ == number]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\\n\")\n",
        "    jobs_as_string = get_job_infos_by_cluster_number(i, samples_job, clustering).to_string(index=True)\n",
        "    # Generate a word cloud image\n",
        "    wordcloud = WordCloud().generate(jobs_as_string)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9vBePKukQT3H"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9vBePKukQT3H"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNrUNu6OQT58"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NNrUNu6OQT58"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfwYTp6O2FTT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Supposons que vos embeddings sont stockés dans des tableaux NumPy\n",
        "# Remplacez ces lignes par le chargement réel de vos données\n",
        "bert_fine_tuning_cv = np.load('path_to_bert_fine_tuning_cv.npy')\n",
        "bert_fine_tuning_job = np.load('path_to_bert_fine_tuning_job.npy')\n",
        "\n",
        "# Combiner les vecteurs de CVs et d'offres d'emploi\n",
        "all_vectors = np.concatenate([bert_fine_tuning_cv, bert_fine_tuning_job], axis=0)\n",
        "\n",
        "# Optionnel : Normaliser les données pour une meilleure performance de K-means\n",
        "scaler = StandardScaler()\n",
        "all_vectors = scaler.fit_transform(all_vectors)\n",
        "\n",
        "# Définir le nombre de clusters souhaité\n",
        "n_clusters = 10  # Vous pouvez ajuster ce nombre selon vos besoins\n",
        "\n",
        "# Appliquer K-means\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "labels = kmeans.fit_predict(all_vectors)\n",
        "\n",
        "# Séparer les labels des CVs et des offres d'emploi\n",
        "cv_labels = labels[:len(bert_fine_tuning_cv)]\n",
        "job_labels = labels[len(bert_fine_tuning_cv):]\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Labels des CVs:\", cv_labels)\n",
        "print(\"Labels des offres d'emploi:\", job_labels)\n"
      ],
      "id": "zfwYTp6O2FTT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDXujSJ935kE"
      },
      "outputs": [],
      "source": [],
      "id": "ZDXujSJ935kE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wnWT5dB2HEA"
      },
      "outputs": [],
      "source": [
        "# Calculer la similarité cosinus\n",
        "similarity_matrix = cosine_similarity(all_vectors)\n",
        "\n",
        "# Examiner les similarités au sein des clusters\n",
        "def cluster_similarities(cluster_label, labels, similarity_matrix):\n",
        "    indices = np.where(labels == cluster_label)[0]\n",
        "    cluster_sim = similarity_matrix[np.ix_(indices, indices)]\n",
        "    return cluster_sim\n",
        "\n",
        "# Exemple d'examen pour le cluster 0\n",
        "cluster_label = 0\n",
        "cluster_sim = cluster_similarities(cluster_label, labels, similarity_matrix)\n",
        "\n",
        "# Afficher les similarités au sein du cluster 0\n",
        "print(f\"Similarités au sein du cluster {cluster_label}:\")\n",
        "print(cluster_sim)\n",
        "\n"
      ],
      "id": "0wnWT5dB2HEA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEfSoZuA394d"
      },
      "outputs": [],
      "source": [
        "# Visualiser les similarités avec une heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cluster_sim, annot=True, cmap='coolwarm', cbar=True)\n",
        "plt.title(f\"Heatmap des similarités au sein du cluster {cluster_label}\")\n",
        "plt.show()"
      ],
      "id": "TEfSoZuA394d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5qBfE-u3_F3"
      },
      "outputs": [],
      "source": [],
      "id": "P5qBfE-u3_F3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mle3CmQr3_Kb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculer le score de silhouette\n",
        "silhouette_avg = silhouette_score(all_vectors, labels)\n",
        "print(f\"Le score de silhouette moyen est de : {silhouette_avg}\")\n",
        "\n",
        "# Calculer les échantillons de silhouette pour chaque point\n",
        "sample_silhouette_values = silhouette_samples(all_vectors, labels)\n",
        "\n",
        "# Calculer l'inertie\n",
        "inertia = kmeans.inertia_\n",
        "print(f\"L'inertie du clustering est de : {inertia}\")\n"
      ],
      "id": "Mle3CmQr3_Kb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMgBBQCO4FoV"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# Réduction de la dimensionnalité avec t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)\n",
        "tsne_results = tsne.fit_transform(all_vectors)\n",
        "\n",
        "# Visualiser les clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1], hue=labels, palette='viridis', legend='full')\n",
        "plt.title('Visualisation des Clusters avec t-SNE')\n",
        "plt.show()\n"
      ],
      "id": "yMgBBQCO4FoV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GpcX8_w4Fr5"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "\n",
        "# Réduction de la dimensionnalité avec UMAP\n",
        "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_results = umap_model.fit_transform(all_vectors)\n",
        "\n",
        "# Visualiser les clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=umap_results[:, 0], y=umap_results[:, 1], hue=labels, palette='viridis', legend='full')\n",
        "plt.title('Visualisation des Clusters avec UMAP')\n",
        "plt.show()\n"
      ],
      "id": "2GpcX8_w4Fr5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qurOruZdAtMU"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer, AutoModel\n",
        "\n",
        "# Charger les embeddings des CVs et des offres d'emploi\n",
        "# Load the model directly from the directory\n",
        "cv_model = AutoModel.from_pretrained(\"/content/drive/MyDrive/projet pfe/bert_fine_tuned\")\n",
        "cv_embeddings = cv_model.embeddings.word_embeddings.weight.data.numpy()  # Extract embeddings\n",
        "\n",
        "job_model = AutoModel.from_pretrained(\"/content/drive/MyDrive/projet pfe/bert_fine_tuned_job\")\n",
        "job_embeddings = job_model.embeddings.word_embeddings.weight.data.numpy()\n",
        "\n",
        "# Combiner les embeddings pour clustering\n",
        "all_embeddings = np.concatenate((cv_embeddings, job_embeddings), axis=0)\n",
        "\n",
        "#all_embeddings = torch.cat((cv_embeddings, job_embeddings), dim=0).numpy()\n",
        "\n",
        "# Appliquer K-means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=0).fit(all_embeddings)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Séparer les labels pour les CVs et les offres d'emploi\n",
        "cv_labels = labels[:len(cvs)]\n",
        "job_labels = labels[len(cvs):]\n"
      ],
      "id": "qurOruZdAtMU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oblBv-PyEXN5"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Sauvegarder le modèle K-means\n",
        "joblib.dump(kmeans, 'kmeans_model.pkl')"
      ],
      "id": "oblBv-PyEXN5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObEye5zD9RDF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculer la similarité cosinus entre les embeddings des CVs et des offres d'emploi\n",
        "similarity_matrix = cosine_similarity(cv_embeddings, job_embeddings)\n",
        "\n",
        "# Exemple de visualisation des similarités\n",
        "import pandas as pd\n",
        "\n",
        "#similarity_df = pd.DataFrame(similarity_matrix, index=[f\"CV {i+1}\" for i in range(len(cvs))],\n",
        "                             #columns=[f\"Job {i+1}\" for i in range(len(jobs))])\n",
        "similarity_df = pd.DataFrame(similarity_matrix,\n",
        "                             index=[f\"CV {i+1}\" for i in range(similarity_matrix.shape[0])],  # Use shape of similarity matrix\n",
        "                             columns=[f\"Job {i+1}\" for i in range(similarity_matrix.shape[1])])  # Use shape of similarity matrix\n",
        "\n",
        "print(similarity_df)\n"
      ],
      "id": "ObEye5zD9RDF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8SZJ0Bw9ROM"
      },
      "outputs": [],
      "source": [
        "# Trouver les meilleures correspondances en fonction des scores de similarité\n",
        "def match_cvs_jobs(similarity_matrix):\n",
        "    matches = []\n",
        "    for i, similarities in enumerate(similarity_matrix):\n",
        "        best_match_idx = similarities.argmax()\n",
        "        matches.append((i, best_match_idx, similarities[best_match_idx]))\n",
        "    return matches\n",
        "\n",
        "matches = match_cvs_jobs(similarity_matrix)\n",
        "print(\"Matching results (CV index, Job index, Similarity):\", matches)\n"
      ],
      "id": "t8SZJ0Bw9ROM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM2ncIRODPuk"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Charger le tokenizer et le modèle BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fonction pour obtenir les embeddings BERT\n",
        "def get_bert_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Charger le modèle de clustering\n",
        "kmeans = KMeans(n_clusters=5)\n",
        "# Charger les clusters (vous devrez avoir sauvegardé les clusters)\n",
        "kmeans.cluster_centers_ = pd.read_pickle('/content/kmeans_model.pkl')\n",
        "\n",
        "# Fonction pour matcher les CVs aux offres d'emploi\n",
        "def match_cvs_jobs(cv_texts, job_texts):\n",
        "    cv_embeddings = get_bert_embeddings(cv_texts)\n",
        "    job_embeddings = get_bert_embeddings(job_texts)\n",
        "    similarity_matrix = cosine_similarity(cv_embeddings, job_embeddings)\n",
        "    matches = []\n",
        "    for i, similarities in enumerate(similarity_matrix):\n",
        "        best_match_idx = similarities.argmax()\n",
        "        matches.append((cv_texts[i], job_texts[best_match_idx], similarities[best_match_idx]))\n",
        "    return matches\n"
      ],
      "id": "SM2ncIRODPuk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4Qlv8rYUDUQ3"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n"
      ],
      "id": "4Qlv8rYUDUQ3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni_3asYUDVRP"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Charger le tokenizer et le modèle BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fonction pour obtenir les embeddings BERT\n",
        "def get_bert_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Fonction pour matcher les CVs aux offres d'emploi\n",
        "def match_cvs_jobs(cv_texts, job_texts):\n",
        "    cv_embeddings = get_bert_embeddings(cv_texts)\n",
        "    job_embeddings = get_bert_embeddings(job_texts)\n",
        "    similarity_matrix = cosine_similarity(cv_embeddings, job_embeddings)\n",
        "    matches = []\n",
        "    for i, similarities in enumerate(similarity_matrix):\n",
        "        best_match_idx = similarities.argmax()\n",
        "        matches.append((cv_texts[i], job_texts[best_match_idx], similarities[best_match_idx]))\n",
        "    return matches\n",
        "\n",
        "# Interface utilisateur Streamlit\n",
        "st.title('Matching de CVs et Offres d\\'Emploi')\n",
        "st.write('Charger les fichiers CSV contenant les CVs et les offres d\\'emploi.')\n",
        "\n",
        "# Charger les fichiers CSV\n",
        "cv_file = st.file_uploader('/content/drive/MyDrive/projet pfe/CvClean.csv', type='csv')\n",
        "job_file = st.file_uploader('/content/drive/MyDrive/projet pfe/JobDescription.csv', type='csv')\n",
        "\n",
        "if cv_file and job_file:\n",
        "    cvs = pd.read_csv(cv_file)\n",
        "    jobs = pd.read_csv(job_file)\n",
        "\n",
        "    # Assurer que les colonnes de texte sont nommées 'text'\n",
        "    cvs['Resume'] = cvs['Resume'].astype(str)\n",
        "    jobs['jobs'] = jobs['jobs'].astype(str)\n",
        "\n",
        "    # Matcher les CVs et les offres d'emploi\n",
        "    matches = match_cvs_jobs(cvs['Resume'].tolist(), jobs['jobs'].tolist())\n",
        "\n",
        "    st.write('Résultats du Matching:')\n",
        "    for cv, job, similarity in matches:\n",
        "        st.write(f'CV: {cv[:200]}... | Offre: {job[:200]}... | Similarité: {similarity:.2f}')\n",
        "\n",
        "# Exécuter l'application avec la commande suivante dans le terminal:\n",
        "# streamlit run app.py\n"
      ],
      "id": "Ni_3asYUDVRP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CltTfv1WDaid"
      },
      "outputs": [],
      "source": [],
      "id": "CltTfv1WDaid"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HrDCReXv_R2P"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HrDCReXv_R2P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7XCiuhKBBaz"
      },
      "outputs": [],
      "source": [],
      "id": "m7XCiuhKBBaz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8faTJrTWBBd5"
      },
      "outputs": [],
      "source": [],
      "id": "8faTJrTWBBd5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xS_BruuBBhH"
      },
      "outputs": [],
      "source": [],
      "id": "-xS_BruuBBhH"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 827.121854,
      "end_time": "2021-12-24T10:25:10.799091",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-12-24T10:11:23.677237",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}