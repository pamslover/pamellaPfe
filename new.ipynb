{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "resumeparser.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "qnAImZj8E0h4",
    "6dpOvwRxxJEH",
    "glH4LZEkyIVj",
    "CIZu7zjFxgEx",
    "ihJo4wAsxr1_",
    "IJ7heBeHx5A4"
   ],
   "private_outputs": true,
   "toc_visible": true,
   "mount_file_id": "15L5pAceN_FPNqX-rGy3OmsTics9JjPVz",
   "authorship_tag": "ABX9TyNk1DAokb0SSGJsXmbIlrjQ"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PACKAGE"
   ],
   "metadata": {
    "id": "oTHZM5b1wfX-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8on3D5JMIKdr"
   },
   "outputs": [],
   "source": [
    "!pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install docx2txt\n"
   ],
   "metadata": {
    "id": "wDJwWlZzIXwL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install --upgrade pyresparser"
   ],
   "metadata": {
    "id": "T86YYfL5J41u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m spacy download en_core_web_md"
   ],
   "metadata": {
    "id": "bMrFO0hX8GZs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m spacy download fr_core_news_md\n",
    "!python -m spacy download en_core_web_md"
   ],
   "metadata": {
    "id": "QGCS56ndbJZ8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "fr_nlp = spacy.load(\"fr_core_news_md\")\n",
    "en_nlp = spacy.load(\"en_core_web_md\")"
   ],
   "metadata": {
    "id": "aXl-XNABbXuR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pdfminer.six"
   ],
   "metadata": {
    "id": "aU0fwLwIkMis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install unidecode"
   ],
   "metadata": {
    "id": "joAD-I1RlxaG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install fitz"
   ],
   "metadata": {
    "id": "9fkwn4llTJ1r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install textacy\n",
    "!pip install langid"
   ],
   "metadata": {
    "id": "93o2m1mEuTGR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.downloader.download('treebank')\n",
    "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "nltk.download ('wordnet')\n"
   ],
   "metadata": {
    "id": "yab2ua4OB83-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import csv\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import requests"
   ],
   "metadata": {
    "id": "BMgCNoB9mNte"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "metadata": {
    "id": "IYd-9Uev4l2F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "AT8I0qd3CxMF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CLASS"
   ],
   "metadata": {
    "id": "TGuJClvtwzOZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## extraction\n"
   ],
   "metadata": {
    "id": "ZthK2907m65a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from io import StringIO\n",
    "import glob\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import docx2txt\n",
    "import re\n"
   ],
   "metadata": {
    "id": "Y9mRUH3BSB0s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import unicodedata\n",
    "\n",
    "class extract_text:\n",
    "\n",
    "\n",
    "    def pdf_extract_text (filename, pages=None):\n",
    "          if not pages:\n",
    "            pagenums=set()\n",
    "          else:\n",
    "            pagenums=set(pages)\n",
    "\n",
    "          output= StringIO()\n",
    "          # data={}\n",
    "          contenue=[]\n",
    "          # name=[]\n",
    "          # for filename in glob.glob ('/content/drive/MyDrive/CV/*.pdf'):\n",
    "          with open (filename, 'rb') as in_file:\n",
    "            if filename.endswith ('.docx') or filename.endswith ('.doc'):\n",
    "              text=docx2txt.process(in_file)\n",
    "              text=unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8') \n",
    "              contenue= ' '.join([str(elem) for elem in (text.split('\\n'))])\n",
    "              text=unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8') \n",
    "            elif filename.endswith ('.pdf'):\n",
    "              parser= PDFParser(in_file)\n",
    "              doc=PDFDocument(parser)\n",
    "              manager= PDFResourceManager()\n",
    "              converter= TextConverter(manager, output, laparams=LAParams())\n",
    "              interpreter= PDFPageInterpreter (manager, converter)\n",
    "              for page in PDFPage.get_pages(in_file, pagenums):\n",
    "                interpreter.process_page(page)\n",
    "              text=output.getvalue()\n",
    "              text= unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8') \n",
    "              contenue= ' '.join([str(elem) for elem in (text.split('\\n'))])\n",
    "              in_file.close()\n",
    "              converter.close()\n",
    "              output.close()\n",
    "            else:\n",
    "              pass\n",
    "          return  contenue\n",
    "\n",
    "    def extract_number(text):\n",
    "          numbers=[]\n",
    "          phone=(re.findall((r'[0-9]?[0-9 (\\)]{8,15}[0-9]') ,str(text)))\n",
    "          if phone:\n",
    "                    number= ''.join(phone[0])\n",
    "                    if text.find(number)>=0 and len(number)<20:\n",
    "                      numbers.append(''.join(str(elem) for elem in number))\n",
    "          return numbers\n",
    "\n",
    "    def extract_date (text):\n",
    "          dates=[]\n",
    "          dates.append(re.findall((r\"\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\") ,text))\n",
    "          dates.append(re.findall((r\"\\w{4,9} ?\\d{2,4}\") ,text))\n",
    "          dates.append(re.findall((r\"\\d{2,4}[ ][-/][ ]\\d{2,4}\") ,text))\n",
    "          \n",
    "          return dates\n"
   ],
   "metadata": {
    "id": "8epyi2BXoj_e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process clean"
   ],
   "metadata": {
    "id": "glH4LZEkyIVj"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SK0LRVZH3p04"
   },
   "source": [
    "import unidecode\n",
    "\n",
    "class cleaner:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "   \n",
    "    def clean_text(sentence:str):\n",
    "        \n",
    "        text = cleaner.remove_special_char(cleaner.extract_only_text(sentence))\n",
    "        return cleaner.remove_double_space(text)\n",
    "    \n",
    "    def remove_double_space(text):\n",
    "         return re.sub(r'([\\s])\\1{1,}|([\\.])\\1{1,}', r'\\1', text)\n",
    "        #  return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
    "\n",
    "    def extract_only_text(text):\n",
    "        # text=unidecode.unidecode(text).lower()\n",
    "        text = re.sub('\\S*www\\S*\\s?', '', str(text))\n",
    "        text = re.sub('\\S*http\\S*\\s?', '', str(text))\n",
    "        # text = re.sub('(\\d+\\.)+\\d*.*','',str(text))\n",
    "        text = re.sub('\\d+','',str(text))\n",
    "        text = re.sub(\"-|_\",'',str(text))\n",
    "        text = re.sub(\"\\n|\\r\",'',str(text))\n",
    "        text = re.sub(\"[0-9]+\",'',str(text))\n",
    "        text= re.sub(r'\\x00|\\uf001','fi',str(text))\n",
    "        text= re.sub(r'cv','',str(text))\n",
    "        text= re.sub(r'\\x0c','',str(text))\n",
    "        text= re.sub(r'curriculum |vitae|curriculum Vitae','',str(text))\n",
    "        # text= re.sub(r'[^www]?[A-Za-z] ','',text)\n",
    "        text= re.sub(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+', '', str(text))\n",
    "        # text= re.sub(r'www[a-z0-9\\.\\-+_]+\\.[a-z]+[/-][a-zA-Z]?[a-z\\-]+', r'', str(text))\n",
    "        text= re.sub(r'\\uf005|\\uf006|\\uf10b|\\ue81e|\\ue811|\\ue80d|\\ue81a|\\ue803', r'', str(text))\n",
    "      \n",
    "        return text\n",
    "    \n",
    "    def remove_special_char(text:str):\n",
    "        return re.sub(r\"[^a-zA-Z0-9\\-“”\\s\\'\\.]\",\" \", text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sys import path\n",
    "from pathlib import Path\n",
    "\n",
    "liste_test = glob.glob ('/content/drive/MyDrive/projet pfe/data/*')\n",
    "liste=[]\n",
    "title=[]\n",
    "Email=[]\n",
    "number=[]\n",
    "for filename in liste_test:\n",
    "        # text = pdf_extract_text (filename)\n",
    "        text = extract_text.pdf_extract_text (filename).lower()\n",
    "        texte=cleaner.clean_text(text)\n",
    "        liste.append(texte)\n",
    "        email= (re.findall((r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+'), str(text)))\n",
    "        Email.append( ' '.join([str(elem) for elem in email]))\n",
    "        number.append( extract_text.extract_number(text))\n",
    "        # liste.append(text)\n",
    "        title.append(Path(filename).stem)\n",
    "\n",
    "\n",
    "data=pd.DataFrame(liste, index=None)\n",
    "data['Candidates']= pd.DataFrame(title)\n",
    "data['Emails']= pd.DataFrame(Email)\n",
    "data['Phone_number']= pd.DataFrame(number)\n",
    "data['Resumes']= pd.DataFrame(liste)\n",
    "\n",
    "data= data.drop(columns=0)\n",
    "\n",
    "data.to_csv('data.csv') \n",
    "  \n",
    "df = pd.read_csv('data.csv', engine='python' ) \n",
    "df= df.drop(columns='Unnamed: 0')\n",
    "df\n",
    "        "
   ],
   "metadata": {
    "id": "Cuso8zBN8T_D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(\"/content/data.csv\")"
   ],
   "metadata": {
    "id": "9tUmCq9MW0hS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "id": "RW7yBPw9KLaW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluator\n",
    "\n"
   ],
   "metadata": {
    "id": "ihJo4wAsxr1_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/LIAAD/yake "
   ],
   "metadata": {
    "id": "UkPA418M7AV7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import yake\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "\n",
    "class LangDetection:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.text=None\n",
    "        self.language=None\n",
    "\n",
    "    def override(self, text):\n",
    "        self.text = text\n",
    "        identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "        self.language  = identifier.classify(self.text)\n",
    "\n",
    "\n",
    "        return self.language\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Detection(text):\n",
    "        identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "        lang = identifier.classify(text)\n",
    "        if \"en\" in lang:\n",
    "                    doc= en_nlp(text)\n",
    "                    language= 'en'\n",
    "        elif \"fr\" in lang:\n",
    "                    doc= fr_nlp(text)\n",
    "                    language= 'fr'\n",
    "        return language\n",
    "\n",
    "\n",
    "\n",
    "class Keywords (LangDetection):\n",
    "\n",
    "      def __init__(self,text):\n",
    "          self.lang= LangDetection()\n",
    "          \n",
    "\n",
    "      def keys (text):\n",
    "          language = LangDetection.Detection(text)\n",
    "          max_ngram_size = 3\n",
    "          deduplication_thresold = 0.9\n",
    "          deduplication_algo = 'seqm'\n",
    "          windowSize = 1\n",
    "          numOfKeywords = 50\n",
    "\n",
    "          custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "          keywords = custom_kw_extractor.extract_keywords(text)\n",
    "          return [kw for kw in keywords]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "z-o2cHlb1qlY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Keywords.keys(text=df[\"Resumes\"][2])"
   ],
   "metadata": {
    "id": "IfFrqghMEZKG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    ""
   ],
   "metadata": {
    "id": "KrZTnbBMKI6b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "QLiD3zakKJHx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "\n",
    "def Document(text):\n",
    "                  # text= extract_text.pdf_extract_text(text)\n",
    "                  identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "                  lang = identifier.classify(text)\n",
    "                  if \"en\" in lang:\n",
    "                    doc= en_nlp(text)\n",
    "                  elif \"fr\" in lang:\n",
    "                    doc= fr_nlp(text)\n",
    "                  return doc\n",
    "\n",
    "def token (sentence):\n",
    "  doc= Document(sentence)\n",
    "  return [token.text for token in doc if (not token.is_stop) ]\n",
    "\n",
    "def stopword (sentence):\n",
    "  sentence = extract_text.pdf_extract_text(sentence).lower()\n",
    "  sentence = cleaner.clean_text(sentence)\n",
    "  doc= Document(sentence)\n",
    "  return [token.lemma_ for token in token(sentence) if (not token.is_punct) ]\n",
    "\n",
    "\n",
    "def token_sentence (sentence):\n",
    "  doc= Document(sentence)\n",
    "  return [token.text for token in doc.sents]\n",
    "\n",
    "def ner (sentence):\n",
    "  doc= Document (sentence)\n",
    "  return [(x.text, x.label_) for x in doc.ents]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "RIjymXVUxjZg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "0Kfsc9mBUd9c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "df['Resum'] = df.Resumes.apply(lambda x: ' '.join(ner(x)))\n",
    "df.head()"
   ],
   "metadata": {
    "id": "SLAscBYphrSu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cseulW-UP8QR"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "class Evaluator:\n",
    "\n",
    "    def computeTFIDF(self, documents):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform(documents)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        df = pd.DataFrame(denselist, columns=feature_names)\n",
    "        df.index = ([f\"TFIDF_{i}\" for i in range(1,len(documents)+1)])\n",
    "\n",
    "        return df.T \n",
    "\n",
    "    def similarity(self,documents):\n",
    "        similarities = {}\n",
    "        for i,document in enumerate(documents,1):\n",
    "            doc = Document(document)\n",
    "            sim = []\n",
    "            for document_ in documents:\n",
    "                sim.append(doc.similarity(Document(document_))) \n",
    "            similarities[f\"Doc_{i}\"] = sim\n",
    "        df = pd.DataFrame(similarities, index=[f\"Doc_{i}\" for i in range(1,len(documents)+1)])\n",
    "        return df\n",
    "\n",
    "    def display_lda(self,lda_model, corpus, id2word):\n",
    "        return\n",
    "        pyLDAvis.enable_notebook()\n",
    "        vis = gensimvis.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "        return vis\n",
    "\n",
    "    def LDA(self,data_words):\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "        corpus = []\n",
    "        \n",
    "        for text in data_words:\n",
    "            new = id2word.doc2bow(text)\n",
    "            corpus.append(new)\n",
    "        num_topic = len(corpus)\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topic,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           eta = [.01]*len(id2word.keys()),\n",
    "                                           alpha=[.01]*num_topic)\n",
    "\n",
    "        \n",
    "\n",
    "        return lda_model ,self.display_lda(lda_model,corpus,id2word)\n",
    "        #lda_model.print_topics()\n",
    "\n",
    "    # def bert (sel, documents):\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test= glob.glob (\"/content/drive/MyDrive/projet pfe/CV/chaima.pdf\")\n",
    "\n",
    "tok=[]\n",
    "for filename in liste_test:\n",
    "  print (evaluator.computeTFIDF)"
   ],
   "metadata": {
    "id": "NXoAYjliPpIM"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}