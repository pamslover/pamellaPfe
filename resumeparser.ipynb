{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qnAImZj8E0h4",
        "6dpOvwRxxJEH",
        "glH4LZEkyIVj",
        "CIZu7zjFxgEx",
        "ihJo4wAsxr1_",
        "IJ7heBeHx5A4"
      ],
      "private_outputs": true,
      "mount_file_id": "15L5pAceN_FPNqX-rGy3OmsTics9JjPVz",
      "authorship_tag": "ABX9TyMd3DnERnCgTt6mB8kcTjmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pamslover/pamellaPfe/blob/main/resumeparser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PACKAGE"
      ],
      "metadata": {
        "id": "oTHZM5b1wfX-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8on3D5JMIKdr"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n"
      ],
      "metadata": {
        "id": "wDJwWlZzIXwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_lg\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "QGCS56ndbJZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "id": "aU0fwLwIkMis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "joAD-I1RlxaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install jsonlines"
      ],
      "metadata": {
        "id": "9fkwn4llTJ1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake"
      ],
      "metadata": {
        "id": "lCzmyefVF6nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy\n",
        "!pip install langid"
      ],
      "metadata": {
        "id": "93o2m1mEuTGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# fr_nlp = spacy.load(\"fr_core_news_lg\")\n",
        "# en_nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "aXl-XNABbXuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "import requests"
      ],
      "metadata": {
        "id": "BMgCNoB9mNte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASS"
      ],
      "metadata": {
        "id": "TGuJClvtwzOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ExtractText and DataFrame\n",
        "\n",
        "in this stage:\n",
        "1. extract the text from the cvs words and PDF\n",
        "2.normalize the text \n",
        "3.perform a first cleaning of our text\n",
        "4.converting characters to lowercases.\n"
      ],
      "metadata": {
        "id": "ZthK2907m65a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "import glob\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "from docx import Document\n",
        "import docx2txt\n",
        "import re\n"
      ],
      "metadata": {
        "id": "Y9mRUH3BSB0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "\n",
        "class extraction:\n",
        "\n",
        "\n",
        "    def pdf_extract_text (filename, pages=None):\n",
        "          if not pages:\n",
        "            pagenums=set()\n",
        "          else:\n",
        "            pagenums=set(pages)\n",
        "\n",
        "          contenue=[]\n",
        "\n",
        "          with open (filename, 'rb') as in_file:\n",
        "            if filename.endswith ('.docx') or filename.endswith ('.doc'):\n",
        "              text=docx2txt.process(in_file)\n",
        "              text=unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8') \n",
        "              contenue= ' '.join([str(elem) for elem in (text.split('\\n'))])\n",
        "            elif filename.endswith ('.pdf'):\n",
        "              parser= PDFParser(in_file)\n",
        "              doc=PDFDocument(parser)\n",
        "              output= StringIO()\n",
        "              manager= PDFResourceManager()\n",
        "              converter= TextConverter(manager, output, laparams=LAParams())\n",
        "              interpreter= PDFPageInterpreter (manager, converter)\n",
        "              for page in PDFPage.get_pages(in_file, pagenums):\n",
        "                interpreter.process_page(page)\n",
        "              text=output.getvalue()\n",
        "              text= unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('UTF-8') \n",
        "              contenue= ' '.join([str(elem) for elem in (text.splitlines())])\n",
        "              in_file.close()\n",
        "              converter.close()\n",
        "              output.close()\n",
        "            else:\n",
        "              pass\n",
        "          return  contenue\n",
        "\n",
        "    def extract_number(text):\n",
        "          numbers=[]\n",
        "          phone=(re.findall((r'[0-9]?[0-9 (\\)]{8,15}[0-9]') ,str(text)))\n",
        "          if phone:\n",
        "                    number= ''.join(phone[0])\n",
        "                    if text.find(number)>=0 and len(number)<20:\n",
        "                      numbers.append(''.join(str(elem) for elem in number))\n",
        "          return numbers\n",
        "\n",
        "    def extract_date (text):\n",
        "          dates=[]\n",
        "          dates.append(re.findall((r\"\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\") ,text))\n",
        "          dates.append(re.findall((r\"\\w{4,9} ?\\d{2,4}\") ,text))\n",
        "          dates.append(re.findall((r\"\\d{2,4}[ ][-/][ ]\\d{2,4}\") ,text))\n",
        "          \n",
        "          return dates\n",
        "\n",
        "\n",
        "    # def extract_name(text):\n",
        "    #   nlp_text = nlp(text)\n",
        "    #   # First name and Last name are always Proper Nouns\n",
        "    #   pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    #   matcher.add('NAME', [pattern], on_match = None)\n",
        "    #   matches = matcher(nlp_text)\n",
        "      \n",
        "    #   for match_id, start, end in matches:\n",
        "    #       span = nlp_text[start:end]\n",
        "    #       return span.text\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "8epyi2BXoj_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK0LRVZH3p04"
      },
      "source": [
        "import unidecode\n",
        "\n",
        "class cleaner:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "   \n",
        "    def clean_text(sentence:str):\n",
        "        \n",
        "        text=unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf8') \n",
        "        text =cleaner.remove_special_char(cleaner.extract_only_text(text))\n",
        "        return cleaner.remove_double_space(text)\n",
        "    \n",
        "    def remove_double_space(text):\n",
        "         return re.sub(r'([\\s])\\1{1,}|([\\.])\\1{1,}', r'\\1', str(text))\n",
        "        #  return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
        "\n",
        "    def extract_only_text(text):\n",
        "        text= text.lower()\n",
        "        text= re.sub(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+', ' ', str(text))\n",
        "        text= re.sub(r'[^ ]+\\.[^ ]+', ' ', str(text))\n",
        "        # text = re.sub('\\S*www\\S*\\s?', ' ', str(text))\n",
        "        text = re.sub('http\\S+\\s*', ' ', str(text))\n",
        "        # text = re.sub('(\\d+\\.)+\\d*.*','',str(text))\n",
        "        text = re.sub('#S+', ' ', str(text))  # remove hashtags\n",
        "        text = re.sub('@S+', '  ', str(text))  # remove mentions\n",
        "        text = re.sub('\\s+', ' ', str(text))  # remove extra whitespace\n",
        "        text = re.sub(\"\\t\",' ',str(text))\n",
        "        text = re.sub('\\n|\\r',' ',str(text))\n",
        "        text = re.sub('[^\\x00-\\x7f]',' ',str(text))\n",
        "        text= re.sub(r'\\x00|\\uf001','fi',str(text))\n",
        "        text= re.sub(r\"cv|'\",' ',str(text))\n",
        "        text = re.sub(r'@+[A-Za-z0-9]+', ' ', str(text)) \n",
        "        text= re.sub(r'\\x0c',' ',str(text))\n",
        "        text= re.sub(r'www[a-z0-9\\.\\-+_]+\\.[a-z]+[/-][a-zA-Z]?[a-z\\-]+', r' ', str(text))\n",
        "        text = text.replace('\\\\xa0', ' ')  \n",
        "        # text= re.sub(r'\\uf005|\\uf006|\\uf10b|\\ue81e|\\ue811|\\ue80d|\\ue81a|\\ue803', r' ', str(text))\n",
        "      \n",
        "        return text\n",
        "    \n",
        "    def remove_special_char(text:str):\n",
        "        return re.sub(r\"[^a-zA-Z0-9\\-“”\\s\\'\\.]\",\" \", str(text))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import path\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class Data:\n",
        "\n",
        "\n",
        "    def dataframe():\n",
        "\n",
        "        liste_test = glob.glob ('/content/drive/MyDrive/projet pfe/data/*')\n",
        "        liste=[]\n",
        "        title=[]\n",
        "        Email=[]\n",
        "        number=[]\n",
        "        for filename in liste_test:\n",
        "                # text = pdf_extract_text (filename)\n",
        "                text = extraction.pdf_extract_text (filename).lower()\n",
        "                texte=cleaner.clean_text(text)\n",
        "                liste.append(texte)\n",
        "                email= (re.findall((r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+'), str(text)))\n",
        "                Email.append( ' '.join([str(elem) for elem in email]))\n",
        "                number.append( extraction.extract_number(text))\n",
        "                # liste.append(text)\n",
        "                title.append(Path(filename).stem)\n",
        "\n",
        "\n",
        "        data=pd.DataFrame(liste, index=None)\n",
        "        data['Candidates']= pd.DataFrame(title)\n",
        "        data['Emails']= pd.DataFrame(Email)\n",
        "        data['Phone_number']= pd.DataFrame(number)\n",
        "        data['Resumes']= pd.DataFrame(liste)\n",
        "\n",
        "        data= data.drop(columns=0)\n",
        "\n",
        "        data.to_csv('data_cv.csv') \n",
        "          \n",
        "        df = pd.read_csv('data_cv.csv', engine='c' ) \n",
        "        df= df.drop(columns='Unnamed: 0')\n",
        "        return df\n",
        "        "
      ],
      "metadata": {
        "id": "Cuso8zBN8T_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= Data.dataframe()\n"
      ],
      "metadata": {
        "id": "RW7yBPw9KLaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jd = pd.read_csv('/content/drive/MyDrive/projet pfe/JobDescription.csv')\n",
        "# "
      ],
      "metadata": {
        "id": "giaQyO2JTS02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jd"
      ],
      "metadata": {
        "id": "tOoteW7WYdrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/projet pfe/data_cv.csv')\n",
        "df = df.drop(columns='Unnamed: 0')"
      ],
      "metadata": {
        "id": "UDfkxd2NgGiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jd['Bullets'] = jd.Bullets.apply(lambda x: cleaner.clean_text(sentence = x))\n",
        "jd['Title'] = jd.Title.apply(lambda x: cleaner.clean_text(sentence = x))"
      ],
      "metadata": {
        "id": "l2w7ggDsTS4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# jd = jd.drop(columns='Unnamed: 0')\n",
        "jd['Bullets'][1]"
      ],
      "metadata": {
        "id": "vFV5YkGUuaAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ycIZoIfWsJWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resumes = df['Resumes'].astype('str').tolist()\n",
        "jobs = jd['Bullets'].astype('str').tolist"
      ],
      "metadata": {
        "id": "F_Jpo72EycTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "In this stage, I followed basic cleaning processes for text analysis which includes:\n",
        "\n",
        "1.remove punctuations.\n",
        "\n",
        "2.tokenize word and sentence.\n",
        "\n",
        "3.remove stopwords\n",
        "\n",
        "4.part of speech and lemmatization\n",
        "\n",
        "5.lang detection"
      ],
      "metadata": {
        "id": "glH4LZEkyIVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langid.langid import LanguageIdentifier, model\n",
        "import langid\n",
        "\n",
        "\n",
        "class LanguageCheck:\n",
        "\n",
        "    # def __init__(self):\n",
        "    #     self.text = text\n",
        "    #     self.language = None\n",
        "\n",
        "    def override(self, text):\n",
        "        self.text = text\n",
        "        identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
        "        self.language, _ = identifier.classify(self.text)\n",
        "\n",
        "        if self.language == \"fr\":\n",
        "          self.language = \"french\"\n",
        "        elif self.language == \"en\":\n",
        "          self.language = \"english\"\n",
        "\n",
        "\n",
        "        return self.language\n"
      ],
      "metadata": {
        "id": "SVIVS5qfHtkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class process:\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "      self.lang,_= LanguageCheck().override(text)\n",
        "      _,self.doc = LanguageCheck().override(text)\n",
        "\n",
        "\n",
        "    def _segment_sentences(self,text):\n",
        "\n",
        "        text = self._clean_sentences(text)\n",
        "        #text = ProcessText.remove_double_space(text.replace(\"\\t\",\" \")) #replace(\"\\n\",\" \").\n",
        "        return [ sentence.text for sentence in self.doc.sents if len(sentence)> 1]\n",
        "\n",
        "\n",
        "    def _clean_sentences(self,sentences):\n",
        "\n",
        "        for i,sent in enumerate(sentences):\n",
        "            sentence = cleaner.clean_text(sent)\n",
        "            sentence = cleaner.remove_double_space(sentence.replace(\"\\n\",\" \"))\n",
        "            if len(sentence) > 1:\n",
        "                sentences[i] = sentence\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def _tokenization(self, text, all_pos= False):\n",
        "\n",
        "        sentences = self._segment_sentences(text)\n",
        "        if sentences!=\"\":\n",
        "            text = \" \".join(sentences)\n",
        "        if not all_pos:\n",
        "            pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB', \"ADV\", \"X\"]  \n",
        "            return [token for token in self.doc if not token.is_stop \n",
        "                    and len(token)>1 \n",
        "                    and token.pos_ in pos_tag]\n",
        "\n",
        "        return[token for token in self.doc if not token.is_stop and len(token)>1 ]\n",
        "\n",
        "\n",
        "    def _token(self,text):\n",
        "\n",
        "        tok=[token.text.lower() for token in self.doc \n",
        "                                      if len(token)>1 and token.isalpha() ]\n",
        "        return tok\n",
        "\n",
        "    \n",
        "    def tokens(self,text):\n",
        "\n",
        "        tokens = self._tokenization(text)\n",
        "        return self._token(tokens)\n",
        "\n",
        "\n",
        "    def lemmatization (self,text): \n",
        "        return [tok.lemma_ for tok in self.tokens(text)]\n",
        "\n",
        "\"\"\" nous avons ajouté les stopwords pour la verfication de similitude entre les cv et offres et tester si l'ajout des stopwords à une importance ou non\"\"\"\n",
        "\n",
        "    def stopword(text):\n",
        "      identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
        "      lang, _ = identifier.classify(text)\n",
        "       if lang == \"fr\":\n",
        "          stopWords = set(stopwords.words('french'))\n",
        "          words = word_tokenize(text)\n",
        "          wordsFiltered = []\n",
        "\n",
        "          for w in words:\n",
        "              if w not in stopWords:\n",
        "                  wordsFiltered.append(w)\n",
        "       \n",
        "       if lan == \"en\":\n",
        "          stopWords = set(stopwords.words('english'))\n",
        "          words = word_tokenize(text)\n",
        "          wordsFiltered = []\n",
        "\n",
        "          for w in words:\n",
        "              if w not in stopWords:\n",
        "                  wordsFiltered.append(w)\n",
        "              \n",
        "      return wordsFiltered\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O-4x_1m7bu5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process().tokens(text)"
      ],
      "metadata": {
        "id": "RyyUYzeDqyVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_string = StringIO()\n",
        "with open('/content/drive/MyDrive/projet pfe/data/C.V.pdf', 'rb') as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    docu = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    for page in PDFPage.create_pages(docu):\n",
        "        interpreter.process_page(page)\n",
        "    text= output_string.getvalue()\n",
        "    text=unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8') \n",
        "    text= ' '.join([elem for elem in (text.split('\\n'))])\n",
        "    in_file.close()\n",
        "    device.close()\n",
        "    output_string.close()\n"
      ],
      "metadata": {
        "id": "bRGpWo3sUTBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluator\n",
        "\n"
      ],
      "metadata": {
        "id": "ihJo4wAsxr1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Keywords:\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        this is the test of my bla bla car\n",
        "        '''\n",
        "        self.text = None\n",
        "        self.lang = \"french\"\n",
        "        self.windowSize = 1\n",
        "        self.array_of_keywords = []\n",
        "        self.numOfKeywords = 50\n",
        "        self.max_ngram_size = 3\n",
        "        self.deduplication_threshold = 0.9\n",
        "        self.deduplication_algo = 'seqm'\n",
        "\n",
        "    def override(self, text=None, language=\"french\", num_of_keywords=50, max_ngram_size=3, deduplication_thresold=0.9):\n",
        "        self.text = text\n",
        "        self.lang = language\n",
        "        self.numOfKeywords = num_of_keywords\n",
        "        self.max_ngram_size = max_ngram_size\n",
        "        self.deduplication_threshold = deduplication_thresold\n",
        "        custom_kw_extractor = yake.KeywordExtractor(lan=self.lang, n=self.max_ngram_size,\n",
        "                                                    dedupLim=self.deduplication_threshold,\n",
        "                                                    dedupFunc=self.deduplication_algo, windowsSize=self.windowSize,\n",
        "                                                    top=self.numOfKeywords, features=None)\n",
        "        self.array_of_keywords = custom_kw_extractor.extract_keywords(text)\n",
        "        return self.array_of_keywords"
      ],
      "metadata": {
        "id": "vRo4mvLpGWdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yake\n",
        "from yake.highlight import TextHighlighter\n",
        "\n",
        "\n",
        "class Keywords:\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        this is the test of my bla bla car\n",
        "        '''\n",
        "        self.text = None\n",
        "        self.lang = \"fr\"\n",
        "        self.windowSize = 1\n",
        "        self.array_of_keywords = []\n",
        "        self.numOfKeywords = 30\n",
        "        self.max_ngram_size = 3\n",
        "        self.deduplication_threshold = 0.9\n",
        "        self.deduplication_algo = 'seqm'\n",
        "\n",
        "    def override(self, text=None, language=\"en\", num_of_keywords=30, max_ngram_size=3, deduplication_thresold=0.9):\n",
        "        self.text = text\n",
        "        self.lang = language\n",
        "        self.numOfKeywords = num_of_keywords\n",
        "        self.max_ngram_size = max_ngram_size\n",
        "        self.deduplication_threshold = deduplication_thresold\n",
        "        custom_kw_extractor = yake.KeywordExtractor(lan=self.lang, n=self.max_ngram_size,\n",
        "                                                    dedupLim=self.deduplication_threshold,\n",
        "                                                    dedupFunc=self.deduplication_algo, windowsSize=self.windowSize,\n",
        "                                                    top=self.numOfKeywords, features=None)\n",
        "        self.array_of_keywords = custom_kw_extractor.extract_keywords(text)\n",
        "        return self.array_of_keywords\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z-o2cHlb1qlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorisation\n",
        "\n",
        "ici, nous allons tester une ensemble d'algo pour la vectorisation de nos cv et offre d'emplois tel que:\n",
        "\n",
        "*word2vec\n",
        "\n",
        "*tf-idf\n",
        "\n",
        "*bac of words\n",
        "\n",
        "et bien d'autres ainsi nous choisirons celui qui s'adapte le mieux à notre probléme.\n"
      ],
      "metadata": {
        "id": "eu7n85QCObTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "enFPT6e5fEb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "corpus= df[\"Resumes\"]\n",
        "vectorizer= TfidfVectorizer()\n",
        "vector=vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vector.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "df.index = ([f\"TFIDF_{i}\" for i in range(1,len(corpus)+1)])\n",
        "print(df.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "g16e-8MwTrNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbHMJEZdpefe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "#on peut utiliser la similarité entre un cv et une offre d'emplois pour ceterminer à quel pourcentage ils sont proche ensuite recuperer \n",
        "# les cv avec un grand pourcentage et les passer ensuite dans reseau de neurone\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "\n",
        "    def __init__(self):\n",
        "      self.doc= process.Doc()\n",
        "\n",
        "    def computeTFIDF(self, corpus):\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vectors = vectorizer.fit_transform(corpus)\n",
        "        feature_names = vectorizer.get_feature_names()\n",
        "        dense = vectors.todense()\n",
        "        denselist = dense.tolist()\n",
        "        df = pd.DataFrame(denselist, columns=feature_names)\n",
        "        df.index = ([f\"TFIDF_{i}\" for i in range(1,len(corpus)+1)])\n",
        "\n",
        "        return df.T\n",
        "\n",
        "\n",
        "\n",
        "    def countvectorizer(self,corpus):\n",
        "      countvectorizer= CountVectorizer()\n",
        "      X=countvectorizer.fit_transform(corpus)\n",
        "      result= X.toarray()\n",
        "      return result\n",
        "\n",
        "\n",
        "    def similarity(self,corpus):\n",
        "        similarities = {}\n",
        "        for i,document in enumerate(corpus,1):\n",
        "            doc=self.doc(document)\n",
        "            sim = []\n",
        "            for document_ in corpus:\n",
        "                sim.append(doc.similarity(self.doc(document_))) \n",
        "            similarities[f\"Doc_{i}\"] = sim\n",
        "        df = pd.DataFrame(similarities, index=[f\"Doc_{i}\" for i in range(1,len(corpus)+1)])\n",
        "        return df\n",
        "\n",
        "\n",
        "    def display_lda(self,lda_model, corpus, id2word):\n",
        "        return\n",
        "        pyLDAvis.enable_notebook()\n",
        "        vis = gensimvis.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
        "        return vis\n",
        "\n",
        "\n",
        "    def LDA(self,data_words):\n",
        "        id2word = corpora.Dictionary(data_words)\n",
        "        corpus = []\n",
        "        \n",
        "        for text in data_words:\n",
        "            new = id2word.doc2bow(text)\n",
        "            corpus.append(new)\n",
        "        num_topic = len(corpus)\n",
        "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topic,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           eta = [.01]*len(id2word.keys()),\n",
        "                                           alpha=[.01]*num_topic)\n",
        "\n",
        "        return lda_model ,self.display_lda(lda_model,corpus,id2word)"
      ],
      "metadata": {
        "id": "1xoH6FIzkPC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')\n"
      ],
      "metadata": {
        "id": "DDFGTxwkjdXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(min_count=1)\n",
        "model.build_vocab(sentences= sentences)  \n",
        "model.train(sentences= sentences, total_examples=model.corpus_count, epochs=model.epochs)  \n"
      ],
      "metadata": {
        "id": "Eo9o8PZzZg5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model  word2vec avec Gensing"
      ],
      "metadata": {
        "id": "CiLeEFfJlGDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.summarization import keywords\n",
        "import matplotlib.pyplot as plt\n",
        "# df =pd.read_csv('data.csv')\n",
        "job = jd['Bullets'].tolist()\n",
        "resume = df['Resumes'].tolist()\n",
        "# positions = df['position'].tolist()\n",
        "\n",
        "from gensim import models\n",
        "docs = []\n",
        "for i in range(len(jd)):\n",
        "    sent = models.doc2vec.LabeledSentence(words = job[i].split(),tags = ['{}_{}'.format(job[i], i)])\n",
        "    #sent = models.doc2vec.LabeledSentence(words = jd[i].split(),tags = ['jd'])\n",
        "    docs.append(sent)\n",
        "    \n",
        "model = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1)\n",
        "model.build_vocab(docs)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train(docs)\n",
        "    model.alpha -= 0.002  # decrease the learning rate`\n",
        "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
        "    \n",
        "with open('resumeconverted.txt','r') as f:\n",
        "    resume = f.read()\n",
        "\n",
        "    \n",
        "from sklearn.manifold import MDS\n",
        "data = []\n",
        "for i in range(len(jd)):\n",
        "    data.append(model.docvecs[i])\n",
        "\n",
        "data.append(model.infer_vector(resume))\n",
        "\n",
        "mds = MDS(n_components=2, random_state=1)\n",
        "pos = mds.fit_transform(data)\n",
        "#print pos\n",
        "xs,ys = pos[:,0], pos[:,1]\n",
        "for x, y in zip(xs, ys):\n",
        "    plt.scatter(x, y)\n",
        "#    plt.text(x, y, name)\n",
        "xs2,ys2 = xs[-1], ys[-1]\n",
        "plt.scatter(xs2, ys2, c='Red', marker='+')\n",
        "plt.text(xs2,ys2,'resume')\n",
        "plt.savefig('distance.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "cos_dist =[]\n",
        "for i in range(len(data)-1):\n",
        "    print(i)\n",
        "    cos_dist.append(float(cosine_distances(model.infer_vector(resume),data[i])))\n",
        "    \n",
        "\n",
        "key_list =[]\n",
        "\n",
        "for j in resume:\n",
        "    key =''\n",
        "    for word in keywords(j).split('\\n'):\n",
        "        key += '{} '.format(word)\n",
        "    key_list.append(key)\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "        'Resumes': resume,\n",
        "        # 'Postition': positions,\n",
        "        'Cosine Distances': cos_dist,\n",
        "        'Keywords': key_list,\n",
        "        'Job Description': job\n",
        "    })\n",
        "z =summary.sort('Cosine Distances', ascending=False)\n",
        "z.to_csv('Summary.csv',encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "LgGpm-CUlbvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "id": "wAIHPle_zAhk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}